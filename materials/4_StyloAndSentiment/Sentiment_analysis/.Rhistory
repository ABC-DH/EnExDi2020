library(stylo)
stylo()
stylo()
stylo()
stylo()
stylo()
setwd("~/EnExDi2020/data")
plays_list <- list.files("corpus", full.names = T)
plays_list
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (via stylo)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
# print progress
print(i)
}
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (via stylo)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
### If we want to run stylo on the texts now saved in the list, we need to "tokenize" them
### i.e. split them into single words
### There is a function in the "stylo" package that does it,
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], language = "French")
# print progress
print(i)
}
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (via stylo)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
### If we want to run stylo on the texts now saved in the list, we need to "tokenize" them
### i.e. split them into single words
### There is a function in the "stylo" package that does it,
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], corpus.lang = "French")
# print progress
print(i)
}
results_stylo <- stylo(gui = FALSE,
corpus.lang="French",
analysis.type="CA",
mfw.min=2000,
mfw.max=2000,
distance.measure="dist.wurzburg",
parsed.corpus = plays_text)
plays_list[i]
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (via stylo)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
### If we want to run stylo on the texts now saved in the list, we need to "tokenize" them
### i.e. split them into single words
### There is a function in the "stylo" package that does it,
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], corpus.lang = "French")
# finally, we have to assign the correct name to the tokenized text
# for doing this, we can re-use the list of files (by deleting the "corpus" at the beginning)
names(plays_text[[i]]) <- gsub(pattern = "corpus/", replacement = "", x = plays_list[i])
# print progress
print(i)
}
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (via stylo)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
### If we want to run stylo on the texts now saved in the list, we need to "tokenize" them
### i.e. split them into single words
### There is a function in the "stylo" package that does it,
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], corpus.lang = "French")
# finally, we have to assign the correct name to the tokenized text
# for doing this, we can re-use the list of files (by deleting the "corpus" at the beginning)
names(plays_text)[i] <- gsub(pattern = "corpus/", replacement = "", x = plays_list[i])
# print progress
print(i)
}
results_stylo <- stylo(gui = FALSE,
corpus.lang="French",
analysis.type="CA",
mfw.min=2000,
mfw.max=2000,
distance.measure="dist.wurzburg",
parsed.corpus = plays_text)
results_stylo$distance.table
rownames(results_stylo$distance.table)
results_stylo$distance.table[1,]
sort(results_stylo$distance.table[1,])
results_stylo$frequencies.0.culling
colnames(results_stylo$frequencies.0.culling)
fou_position <- which(colnames(results_stylo$frequencies.0.culling) == "fou")
fou_position
fou_position
sort(results_stylo$frequencies.0.culling[,fou_position], decreasing = T)
Chosen_texts <- which(grepl("SCUDERY", names(plays_text)))
Chosen_texts
Chosen_texts
primary_set <- plays_text[Chosen_texts]
secondary_set <- plays_text[-Chosen_texts]
oppose(primary.corpus = primary_set, secondary.corpus = secondary_set)
setwd("~/ownCloud/EnExDi2020_datasets/Sentiment_analysis")
data <- xmlTreeParse("fr-sentiment_lexicon.lmf")
library(XML)
data <- xmlTreeParse("fr-sentiment_lexicon.lmf")
entries <- xmlElementsByTagName(data$doc$children[[1]], "LexicalEntry", recursive = T)
full_lemmas <- character()
full_polarities <- character()
full_confidences <- numeric()
for(entry in entries){
lemma <- xmlChildren(entry)[[1]]
sense <- xmlChildren(entry)[[2]]
polarity <- xmlChildren(sense)[[2]]
confidence <- xmlChildren(sense)[[1]]
if(length(xmlAttrs(lemma))==0){
full_lemmas <- c(full_lemmas, NA)
print("no lemma")
}
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
if(length(xmlAttrs(polarity))==0){
full_polarities <- c(full_polarities, NA)
print("no polarity")
}
full_polarities <- c(full_polarities, xmlAttrs(polarity))
if(length(xmlAttrs(confidence))==0){
full_confidences <- c(full_confidences, NA)
print("no confidence")
}
full_confidences <- c(full_confidences, as.numeric(xmlAttrs(confidence)[[1]]))
}
entry <- entries[[1]]
lemma <- xmlChildren(entry)[[1]]
lemma
sense <- xmlChildren(entry)[[2]]
sense
polarity <- xmlChildren(sense)[[2]]
polarity
entry
sense <- xmlChildren(entry)[[2]]
sense
polarity <- xmlChildren(sense)[[3]]
polarity
confidence <- xmlChildren(sense)[[1]]
confidence
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
full_lemmas
xmlAttrs(polarity)
as.numeric(xmlAttrs(confidence)[[1]])
confidence
xmlAttrs(confidence)[[1]]
xmlAttrs(confidence)[[2]]
entries <- xmlElementsByTagName(data$doc$children[[1]], "LexicalEntry", recursive = T)
full_lemmas <- character()
full_polarities <- character()
full_confidences <- numeric()
for(entry in entries){
lemma <- xmlChildren(entry)[[1]]
sense <- xmlChildren(entry)[[2]]
polarity <- xmlChildren(sense)[[3]]
confidence <- xmlChildren(sense)[[1]]
if(length(xmlAttrs(lemma))==0){
full_lemmas <- c(full_lemmas, NA)
print("no lemma")
}
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
if(length(xmlAttrs(polarity))==0){
full_polarities <- c(full_polarities, NA)
print("no polarity")
}
full_polarities <- c(full_polarities, xmlAttrs(polarity))
if(length(xmlAttrs(confidence))==0){
full_confidences <- c(full_confidences, NA)
print("no confidence")
}
full_confidences <- c(full_confidences, as.numeric(xmlAttrs(confidence)[[2]]))
}
OpeNER <- data.frame(word = full_lemmas, value = full_polarities, confidence = full_confidences, stringsAsFactors = F)
full_polarities
confidence
full_confidences
entries[[12791]]
entries[[12790]]
entries[[12789]]
entries[[12790]]
entries[[12792]]
entries[[12795]]
entries[[12895]]
entries[[12790]]
entries[[12789]]
entries <- entries[1:12789]
full_lemmas <- character()
full_polarities <- character()
full_confidences <- numeric()
for(entry in entries){
lemma <- xmlChildren(entry)[[1]]
sense <- xmlChildren(entry)[[2]]
polarity <- xmlChildren(sense)[[3]]
confidence <- xmlChildren(sense)[[1]]
if(length(xmlAttrs(lemma))==0){
full_lemmas <- c(full_lemmas, NA)
print("no lemma")
}
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
if(length(xmlAttrs(polarity))==0){
full_polarities <- c(full_polarities, NA)
print("no polarity")
}
full_polarities <- c(full_polarities, xmlAttrs(polarity))
if(length(xmlAttrs(confidence))==0){
full_confidences <- c(full_confidences, NA)
print("no confidence")
}
full_confidences <- c(full_confidences, as.numeric(xmlAttrs(confidence)[[2]]))
}
full_lemmas <- character()
full_polarities <- character()
full_confidences <- numeric()
for(entry in entries){
lemma <- xmlChildren(entry)[[1]]
sense <- xmlChildren(entry)[[2]]
polarity <- xmlChildren(sense)[[3]]
confidence <- xmlChildren(sense)[[1]]
if(length(xmlAttrs(lemma))==0){
full_lemmas <- c(full_lemmas, NA)
print("no lemma")
}
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
if(length(xmlAttrs(polarity))==0){
full_polarities <- c(full_polarities, NA)
print("no polarity")
}
full_polarities <- c(full_polarities, xmlAttrs(polarity))
if(length(xmlAttrs(confidence))==0){
full_confidences <- c(full_confidences, NA)
print("no confidence")
}
full_confidences <- c(full_confidences, as.numeric(xmlAttrs(confidence)[[2]]))
}
OpeNER <- data.frame(word = full_lemmas, value = full_polarities, confidence = full_confidences, stringsAsFactors = F)
View(OpeNER)
OpeNER <- OpeNER[-which(is.na(OpeNER$value)),]
OpeNER <- data.frame(word = full_lemmas, value = full_polarities, confidence = full_confidences, stringsAsFactors = F)
OpeNER <- OpeNER[which(!is.na(OpeNER$value)),]
OpeNER <- OpeNER[-which(OpeNER$value == "neutral"),]
OpeNER$value[which(OpeNER$value == "positive")] <- 1
OpeNER$value[which(OpeNER$value == "negative")] <- -1
OpeNER$value <- as.numeric(OpeNER$value)*OpeNER$confidence
OpeNER[which(is.na(OpeNER$value)),]
OpeNER <- OpeNER[which(!is.na(OpeNER$value)),]
OpeNER$confidence <- NULL
View(OpeNER)
View(OpeNER)
OpeNERaggr <- aggregate(word~value,data=OpeNER,FUN=sum)
class(OpeNER$value)
OpeNERaggr <- aggregate(OpeNER$value, by=OpeNER$word, sum)
OpeNERaggr <- aggregate(OpeNER['value'], by=OpeNER['word'], sum)
View(OpeNERaggr)
OpeNER <- aggregate(OpeNER['value'], by=OpeNER['word'], sum)
View(OpeNER)
OpeNER <- OpeNER[-1:13,]
OpeNER <- OpeNER[-(1:13),]
double_word <- which(grepl(pattern = " ", x = OpeNER$word))
OpeNER <- OpeNER[-double_word,]
View(OpeNER)
save(OpeNER, file = "OpeNER-fr.RData")
library(syuzhet)
text <- readLines("texts_eng/Austen_Pride.txt")
text <- paste(text, collapse = " ")
sentences_vector <- get_sentences(text)
syuzhet_vector <- get_sentiment(sentences_vector, method="syuzhet")
syuzhet_emotions <- get_nrc_sentiment(sentences_vector)
syuzhet_emotions$disgust
head(syuzhet_emotions$anger)
head(syuzhet_emotions$anticipation)
simple_plot(syuzhet_emotions$anger, title = "Trust in Pride and Prejudice")
simple_plot(syuzhet_emotions$trust, title = "Trust in Pride and Prejudice")
load("OpeNER-fr.RData")
View(OpeNER)
if (!require("udpipe")) install.packages("udpipe")
plays_list <- list.files("corpus", full.names = T)
plays_list
setwd("~/ownCloud/EnExDi2020_datasets/Stylometry")
plays_list <- list.files("corpus", full.names = T)
plays_list
plays_text <- list()
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (with stylo's function)
loaded_file <- scan("corpus/BOYER_AGAMEMNON.xml", what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
# If we want to run stylo on the texts now saved in the list, we need to "tokenize" them (i.e. split them into single words)
# There is a function in the "stylo" package that does it:
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], corpus.lang = "French")
# then add correct names to the different texts in the list
# (we can re-use the names saved in the list_files variable, by deleting the "corpus/" at the beginning)
names(plays_text)[i] <- gsub(pattern = "corpus/", replacement = "", x = plays_list[i])
# print progress
print(i)
}
head(plays_text[[1]])
plays_text[[1]]
names(plays_text)
head(plays_text[[1]])
head(plays_text[[2]])
plays_text <- list()
for(i in 1:length(plays_list)){
# read xml (with stylo's function)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside the list
plays_text[[i]] <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
# If we want to run stylo on the texts now saved in the list, we need to "tokenize" them (i.e. split them into single words)
# There is a function in the "stylo" package that does it:
plays_text[[i]] <- stylo::txt.to.words.ext(plays_text[[i]], corpus.lang = "French")
# then add correct names to the different texts in the list
# (we can re-use the names saved in the list_files variable, by deleting the "corpus/" at the beginning)
names(plays_text)[i] <- gsub(pattern = "corpus/", replacement = "", x = plays_list[i])
# print progress
print(i)
}
names(plays_text)
head(plays_text[[1]])
head(plays_text[[2]])
results_stylo <- stylo(gui = FALSE,
corpus.lang="French",
analysis.type="CA",
mfw.min=2000,
mfw.max=2000,
distance.measure="dist.wurzburg",
parsed.corpus = plays_text)
library(stylo)
results_stylo <- stylo(gui = FALSE,
corpus.lang="French",
analysis.type="CA",
mfw.min=2000,
mfw.max=2000,
distance.measure="dist.wurzburg",
parsed.corpus = plays_text)
results_stylo <- stylo(gui = FALSE,
corpus.lang="French",
analysis.type="CA",
mfw.min=2000,
mfw.max=2000,
distance.measure="dist.wurzburg",
parsed.corpus = plays_text)
results_stylo$distance.table
rownames(results_stylo$distance.table)
results_stylo$distance.table[1,]
sort(results_stylo$distance.table[1,])
results_stylo$frequencies.0.culling
colnames(results_stylo$frequencies.0.culling)
fou_position <- which(colnames(results_stylo$frequencies.0.culling) == "fou")
fou_position
sort(results_stylo$frequencies.0.culling[,fou_position], decreasing = T)
Chosen_texts <- which(grepl("SCUDERY", names(plays_text)))
Chosen_texts
primary_set <- plays_text[Chosen_texts]
secondary_set <- plays_text[-Chosen_texts]
oppose(primary.corpus = primary_set, secondary.corpus = secondary_set)
oppose(primary.corpus = primary_set, secondary.corpus = secondary_set)
udpipe::udpipe_download_model(language = "french")
setwd("~/ownCloud/EnExDi2020_datasets/Sentiment_analysis")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.4-190531.udpipe")
library(stylo)
plays_list <- list.files("corpus", full.names = T)
plays_list
dir.create("texts_fr")
paste("texts_fr", gsub(pattern = "corpus/", replacement = "", x = plays_list[i]), sep = ""))
paste("texts_fr", gsub(pattern = "corpus/", replacement = "", x = plays_list[i]), sep = "")
new_title <- paste("texts_fr/", gsub(pattern = "corpus/", replacement = "", x = plays_list[i]), sep = "")
new_title
new_title <- gsub(".xml", ".txt", new_title)
new_title
for(i in 1:length(plays_list)){
# read xml (with stylo's function)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside a provisional variable
plays_text <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
# write the text to a new file
# first, define a new title
new_title <- paste("texts_fr/", gsub(pattern = "corpus/", replacement = "", x = plays_list[i]), sep = "")
new_title <- gsub(".xml", ".txt", new_title)
cat(plays_list, file = new_title)
# print progress
print(i)
}
i=1
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
plays_text <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
length(plays_text)
x <- udpipe_annotate(udmodel_french, x = plays_text)
x <- as.data.frame(x)
View(x)
for(i in 1:length(plays_list)){
# print progress
print(i)
# read xml (with stylo's function)
loaded_file <- scan(plays_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE)
# save the texts (without markup) inside a provisional variable
plays_text <- stylo::delete.markup(loaded_file, markup.type = "xml.drama")
# print progress
print("Text uploaded")
# annotate the text using udpipe
x <- udpipe_annotate(udmodel_french, x = plays_text)
x <- as.data.frame(x)
# print progress
print("Text annotated")
# define a new title for the output text
new_title <- paste("texts_fr/", gsub(pattern = "corpus/", replacement = "", x = plays_list[i]), sep = "")
new_title <- gsub(".xml", ".txt", new_title)
# save the lemmatized text (sentence by sentence)
cat(x$lemma[1], "", file = new_title)
for(i in 2:length(x$token_id)){
if(x$sentence_id[i] != x$sentence_id[i-1])
cat("\n", file = new_title, append = T)
if(is.na(x$lemma[i]))
next
cat(x$lemma[i], "", file = new_title, append = T)
}
# print progress
print("Text saved!")
print("")
}
load("OpeNER-fr.RData")
View(OpeNER)
French_text <- readLines("texts_fr/CORNEILLEP_ANDROMEDE.txt")
head(French_text)
syuzhet_vector_French <- get_sentiment(French_text, method = "custom", lexicon = OpeNER)
summary(syuzhet_vector_French)
my_title = gsub(pattern = "corpus/", replacement = "", x = plays_list[i])
my_title = gsub(".txt", "", my_title)
my_title
my_title = "Andromède de Pierre Corneille"
simple_plot(syuzhet_vector_French, title = my_title)
simple_plot(syuzhet_vector_French, title = my_title)
png("Androméde_Corneille_simple_plot.png", height = 900, width = 1600, res = 100)
simple_plot(syuzhet_vector_French, title = my_title)
dev.off()
French_text <- readLines("texts_fr/MOLIERE_MISANTHROPE.txt")
head(French_text)
syuzhet_vector_French <- get_sentiment(French_text, method = "custom", lexicon = OpeNER)
summary(syuzhet_vector_French)
# Prepare OpeNER dictionary
# Call the library (or install it if not present...)
if (!require("XML")) install.packages("XML")
data <- xmlTreeParse("OpeNER-fr-sentiment_lexicon.lmf")
entries <- xmlElementsByTagName(data$doc$children[[1]], "LexicalEntry", recursive = T)
# consider just words with sentiment (not intensifiers and shifters)
entries <- entries[1:12789]
full_lemmas <- character()
full_polarities <- character()
full_confidences <- numeric()
# main iteration on the XML file
for(entry in entries){
lemma <- xmlChildren(entry)[[1]]
sense <- xmlChildren(entry)[[2]]
polarity <- xmlChildren(sense)[[3]]
confidence <- xmlChildren(sense)[[1]]
if(length(xmlAttrs(lemma))==0){
full_lemmas <- c(full_lemmas, NA)
print("no lemma")
}
full_lemmas <- c(full_lemmas, xmlAttrs(lemma))
if(length(xmlAttrs(polarity))==0){
full_polarities <- c(full_polarities, NA)
print("no polarity")
}
full_polarities <- c(full_polarities, xmlAttrs(polarity))
if(length(xmlAttrs(confidence))==0){
full_confidences <- c(full_confidences, NA)
print("no confidence")
}
full_confidences <- c(full_confidences, as.numeric(xmlAttrs(confidence)[[2]]))
}
# save as dataframe
OpeNER <- data.frame(word = full_lemmas, value = full_polarities, confidence = full_confidences, stringsAsFactors = F)
# remove NAs
OpeNER <- OpeNER[which(!is.na(OpeNER$value)),]
# convert labels to values
OpeNER <- OpeNER[-which(OpeNER$value == "neutral"),]
OpeNER$value[which(OpeNER$value == "positive")] <- 1
OpeNER$value[which(OpeNER$value == "negative")] <- -1
OpeNER$value <- as.numeric(OpeNER$value)*OpeNER$confidence # final value is calculated as sentiment*confidence
OpeNER <- OpeNER[which(!is.na(OpeNER$value)),]
OpeNER$confidence <- NULL
# aggregate multiple entries for the same word (different meanings are considered in OpeNER: here everything is reduced to a single value)
OpeNER <- aggregate(OpeNER['value'], by=OpeNER['word'], mean)
# remove numbers and multi-word expressions
OpeNER <- OpeNER[-(1:13),]
double_word <- which(grepl(pattern = " ", x = OpeNER$word))
OpeNER <- OpeNER[-double_word,]
# save all
save(OpeNER, file = "OpeNER-fr.RData")
load("OpeNER-fr.RData")
View(OpeNER)
French_text <- readLines("texts_fr/MOLIERE_MISANTHROPE.txt")
head(French_text)
syuzhet_vector_French <- get_sentiment(French_text, method = "custom", lexicon = OpeNER)
summary(syuzhet_vector_French)
my_title = "MOLIERE_MISANTHROPE"
simple_plot(syuzhet_vector_French, title = my_title)
png("MOLIERE_MISANTHROPE_simple_plot.png", height = 900, width = 1600, res = 100)
simple_plot(syuzhet_vector_French, title = my_title)
dev.off()
my_title = "Le Misanthrope de Molière"
simple_plot(syuzhet_vector_French, title = my_title)
