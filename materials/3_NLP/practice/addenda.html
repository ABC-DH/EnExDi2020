
				<!--*****************************************
 				* SHOWCASE
 				******************************************-->

				<section> 
					<section>
						<h1>Aperitivo</h1>
					</section>
					<section>
						<h3>Trattamento Automatico del Linguaggio</h3>
						<p>Greta Franzini<br />
						Assegnista di Ricerca</p>
						<p>Università Cattolica del Sacro Cuore, Milano</p>
						<img src="./images/unicatt-logo.png" style="border: none; box-shadow: none !important; width: 150px">
						<img src="./images/lila-logo-transp.png" style="border: none; box-shadow: none !important; width: 120px">
					</section>

					<section>
						<h3>Trattamento Automatico del Linguaggio (TAL)</h3>
						<p>Processo di trattamento automatico mediante un calcolatore elettronico delle informazioni scritte o parlate in una lingua naturale (moderna o antica).</p>

						<p class="fragment">TAL ≠ LC</p>

						<h3 class="fragment">Linguistica computazionale (LC)</h3>
						<p class="fragment">Sviluppo di formalismi descrittivi del funzionamento di una lingua naturale, che siano tali da poter essere trasformati in programmi eseguibili da computer.</p>
						
						
						<aside class="notes">
							Il Trattamento Automatico del Linguaggio è il processo di trattamento automatico mediante un calcolatore elettronico della informazioni scritte o parlate in una lingua naturale (sia essa moderna o antica/morta).
							TAL viene spesso confuso con la Linguistica Computazionale. Sebbene i due campi spesso si sovrappongano, il TAL è più ingeneristico, nel senso che si occupa di sviluppare tecnologie in grado di analizzare o generare/riprodurre una linguaggio, mentre la LC è più teorico-scientifico, volta a studiare e scoprire fenomeni linguistici.
						</aside>
					</section>

					<section>
						<h3>LC + TAL</h3>
						<p>Sviluppo di metodi e strumenti in grado di:</p>
						<ul style="font-size: 20pt">
							<li class="fragment">articolare e decodificare i suoni di una lingua: <span style="background-color: black; color: white;">&nbsp;fonetica, fonologia, prosodia, etc.&nbsp;</span>
							<li class="fragment">conoscere la struttura e l'organizzazione delle parole di una lingua: <span style="background-color: black; color: white;">&nbsp;lessico e morfologia&nbsp;</li>
							<li class="fragment">comporre le parole in espressioni linguistiche complesse: <span style="background-color: black; color: white;">&nbsp;sintassi&nbsp;</li>
							<li class="fragment">assegnare significati alle espressioni linguistiche semplici e complesse: <span style="background-color: black; color: white;">&nbsp;semantica&nbsp;</span></li>
							<li class="fragment">usare le frasi nei contesti, situazioni e modi appropriati agli scopi comunicativi: <span style="background-color: black; color: white;">&nbsp;pragmatica&nbsp;</span></li>
						</ul>


					<aside class="notes">
						TAL e LC si propongono di sviluppare metodi e strumenti in grado di: 						
					</aside>
					</section>

					<section>
						<h1>Esempi</h1>
						<aside class="notes">
							La LC e il TAL stanno alla base dell'analisi testuale e ci permettono di fare tante cose bellissime. 
							Vi riporto qui molto brevemente alcuni esempi di LC e NLP tratti dal mio lavoro, passato e attuale.
						</aside>
					</section>

					<!-- TRACER -->
					<!--<section>
						<br /><br />
						<img src="./images/tracer-logo.png" style="border: none; box-shadow: none !important; width: 400px;">
					</section>
					<section>
						<img src="./images/tracer-logo.png" style="border: none; box-shadow: none !important; width: 200px;">
						<ul>
							<li>Strumento di identificazione automatica del riuso testuale</li>
							<li>Citazioni <em>verbatim</em> e parafrasi</li>
							<li>Supporto filologico per analisi a larga scala</li>
						</ul>
						<br /><br />
						<p style="font-size: 16pt;"><i class="fas fa-paperclip"></i> Franzini, G., Passarotti, M., Moritz, M., Büchler, M. (2018) 'Using and evaluating TRACER for an Index fontium computatus of the Summa contra Gentiles of Thomas Aquinas', <em>Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)</em>. Torino, December 2018, 10-12. Available at: <a href="http://ceur-ws.org/Vol-2253/paper22.pdf" target="_blank" title="Opens in new tab">http://ceur-ws.org/Vol-2253/paper22.pdf</a>
						</p>
					
					</section>	-->



					<!-- VERBA BESTIAE-->
					<section>
						<br />
						<img src="./images/verba-bestiae.png" style="border: none; box-shadow: none !important; width: 700px;"><br />
						<img src="./images/metal-hand-bn.png" style="border: none; box-shadow: none !important; width: 200px;">
					
						<aside class="notes">
							Un progetto per cui molto probabilmente io e i miei colleghi in Cattolica verremo licenziati è Verba Bestiae, le "parole della bestia".
						</aside>
					</section>

					<section>
						<img src="./images/verba-bestiae.png" style="border: none; box-shadow: none !important; width: 500px;">
							<ol>
								<li>Utilizzo del latino nella musica Heavy Metal</li>
									<ul style="font-size: 20pt">
										<li>Studio linguistico: e.g. correttezza, originalità</li>
									</ul>
								<li>Identificazione automatica del riuso testuale</li>
									<ul style="font-size: 20pt">
										<li>Dataset: 100 canzoni vs. Vulgata</li>
										<li>Obiettivo: quantificare testo riutilizzato</li>
										<li>Metodo: <a href="https://www.etrap.eu/research/tracer/" target="_blank" title="Opens in new tab">TRACER</a> (strumento TAL) e analisi manuale</li>
										<li>Futuro: estensione ad altre canzoni e opere latine</li>
									</ul>
							</ol>
							<br />
							<p style="font-size: 16pt"><i class="fas fa-paperclip"></i> Cecchini, F. M., Franzini, G., Passarotti, M. (forthcoming), 'Verba Bestiae: How Latin conquered Heavy Metal', <em>Multilingual Metal: Sociocultural, Linguistic and Literary Perspectives on Heavy Metal Lyrics</em> (Emerald Studies in Metal Music and Culture). Emerald Publishing.</p>

						
						<aside class="notes">
							Il progetto si propone di studiare l'utilizzo del latino nella musica heavy metal in maniera semi-automatica. Come primo esperimento, abbiamo selezionato 100 brani/canzoni di 100 band diverse contenenti testo in latino e abbiamo fatto due tipi di analisi: 
							- una sul lessico per verificarne l'originalità (e la correttezza!), 
							- e una volta a individuare e quantificare il riuso della Vulgata. 
							Come prossimo passo estenderemo lo studio del riuso testuale ad altri testi letterari latini, fra cui la Clavicula Salomonis, un testo magico (=grimorio) nonché uno dei + famosi libri di demonologia, che trascriveremo da un manoscritto del 1530.
						</aside>
					</section>	

				

					<!--HARRY POTTER-->
					<section>
						<br /><br />
						<img src="./images/harry-potter.png" style="border: none; box-shadow: none !important; width: 600px;">
					</section>
					<section>
						<img src="./images/harry-potter.png" style="border: none; box-shadow: none !important; width: 300px;">
						<ul>
							<li><a href="http://www.etrap.eu/tutorials/2018-antwerp/" target="_blank" title="Opens in new tab">TRACER tutorial</a>, Anversa, marzo 2018</li>
							<li>Dataset: romanzi e sottotitoli dei film (inglese)</li>
							<li>Obiettivo: quantificare il riuso testuale e il grado di attinenza ai romanzi</li>
						</ul>
					</section>
					<section>
						<img src="./images/harry-potter.png" style="border: none; box-shadow: none !important; width: 300px;">
						<img src="./images/harry-potter-1.png" style="border: none; box-shadow: none !important; width: 100%;">
					</section>
					<section>
						<img src="./images/harry-potter.png" style="border: none; box-shadow: none !important; width: 300px;">
						<img src="./images/harry-potter-2.png" style="border: none; box-shadow: none !important; width: 100%;">
					</section>
					<section>
						<img src="./images/harry-potter.png" style="border: none; box-shadow: none !important; width: 300px;">
						<img src="./images/harry-potter-3.png" style="border: none; box-shadow: none !important; width: 90%;">
					</section>
				


					<!-- LATIN IN FILMS & SERIES -->
					<section>
						<br /><br />
						<img src="./images/opensubtitles.png" style="border: none; box-shadow: none !important; width: 400px;">
					</section>
					<section>
						<img src="./images/opensubtitles.png" style="border: none; box-shadow: none !important; width: 200px;">
						<ul>
							<li>Dataset: 4,957,376 file di sottotitoli (luglio 2019)</li>
							<li>Obiettivi:</li> 
								<ol style="font-size: 20pt">
									<li>language detector: riconoscimento automatico della lingua latina</li>
									<li>individuare e quantificare il latino nei sottotitoli dei film e delle serie TV</li>
								</ol>
							<li>Metodo: machine learning</li>
						</ul>
					</section>


					<!--POLISENSORIALITÀ-->
					<section>
						<img src="./images/enexdi.png" style="border: none; box-shadow: none !important; width: 500px;">
						<ul>
							<li><a href="https://enexdi.sciencesconf.org/" target="_blank" title="Opens in new tab">Scuola invernale</a>: Poitiers, gennaio 2019</li>
							<li>Partecipante: Céline Roussel, dottoranda in letteratura comparata presso la Sorbonne Université</li>
							<li>Progetto: analisi lessico-semantica di opere francesi di autori non vedenti per misurare l'intensità sensoriale</li>
							<!--<li>1) Create (lemmatised) word lists for each sense with the help of French WordNet and use this list as the base lexicon of sentiment analysis tools to detect the intensity of a sense in a text; 2) encode the text and create schema for this</li>-->
						</ul>
					</section>
				</section>

				<!--*****************************************
 				* OTHER USEFUL COMMANDS
 				******************************************-->

				<section> 
					<section>
						<h1>Addenda</h1>
					</section>
					
					<section>
						<h2>Tokenizzazione con la riga di comando</h2>
						<p>Dato un file di testo, produrre tokens e la loro frequenza (ordinati per frequenza decrescente)</p>

						<pre><code class="nohighlight">tr 'A-Z' 'a-z' < INPUT-FILE.txt | tr -sc '[:alpha:]' '\n' | sort | uniq -c | sort -n -r > OUTPUT-FILE.txt</code></pre>

						<ul style="font-size: 20pt;">
							<li><span style="color: white; background-color: black; font-family: monospace; font-size: 20pt">&nbsp;tr 'A-Z' 'a-z'&nbsp;</span> trasforma maiuscole in minuscole</li>
							<li><span style="color: white; background-color: black; font-family: monospace; font-size: 20pt">&nbsp;tr -sc '[:alpha:]' '\n'&nbsp;</span> trasforma tutto ciò che non è una lettera in un invio (new line)</li>
							<li><span style="color: white; background-color: black; font-family: monospace; font-size: 20pt">&nbsp;sort | uniq -c&nbsp;</span> elenca alfabeticamente e conta ogni type</li>
							<li><span style="color: white; background-color: black; font-family: monospace; font-size: 20pt">&nbsp;sort -n -r&nbsp;</span> elenca i conteggi (numericamente) in modo descrescente </li>
						</ul>

					</section>

					<section>
						<h2>MAIUSCOLO > minuscolo</h2>
						<p>Per trasformare una lista di file da maiuscolo in minuscolo:</p>
						<br />
						<pre><code class="nohighlight">for i in $(ls | grep [A-Z]); do mv -i $i `echo $i | tr 'A-Z' 'a-z'`; done</code></pre>
					</section>

					<section>
						<h2>Rimozione markup XML</h2>
						<p>Per rimuovere tutti i tag XML da un file:</p>
						<br />
						<pre><code class="nohighlight">cat filename.xml | sed 's/\<[^<>]*\>//g' > filename.txt</code></pre>
					</section>

					<section>
						<h2>Comandi misti</h2>

						<ul>
							<li><code>grep 'hello' filename</code> to extract all instances of a word in a file</li>
							<li><code>egrep 'hello there' filename</code> to search for a specific string of text</li>
							<li><code>tail filename</code> to return the last 10 lines of the file</li>
							<li><code>tail -20 filename</code> to return the last 20 lines of the file</li>
							<li><code>head filename</code> to return the first 10 lines of the file</li>
							<li><code>head -20 filename</code> to return the first 20 lines of the file</li>
							<li><code>top</code> to view all running processes on a machine</li>
							<li><code>df</code> "Disk Free", to reveal amount of storage space in the current directory</li>
						</ul>

					</section>


					<!--<section>
					<section>
						<h2>2. PoS-tagging & lemmatisation</h2>
						<p style="font-size: 22pt">Process of grouping together the inflected forms of a word so they can be analysed as a single item, known as <span style="color: #04a777">lemma</span> (or base-form).</p>
						<p style="font-size: 22pt">Lemmatisation depends on correctly identifying the part-of-speech and meaning of a word in a sentence (context).</p>
					</section>

					<section>
						<h2>Types of lemmatisation</h2>
						<ol style="font-size: 22pt">
							<li>Rule-based</li>
							<ul>
								<li>Dictionary-based (rule?)</li>
									<ul>
										<li>E.g. LEMLAT</li>
									</ul>
							</ul>
							<li>Data-Driven, probabilistic (machine learning)</li>
							<ul>
								<li><strong>Supervised</strong>: based on human-annotated texts (training data).</li>
									<ul>
										<li>E.g. <a href="http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/" target="_blank" title="Opens in new tab">TreeTagger</a> (Binary decision tree)</li>
										<li>E.g. <a href="https://sites.google.com/site/morfetteweb/home" target="_blank" title="Opens in new tab">Morfette</a> (Edit-Tree induction)</li>
										<li>Neural string transduction</li>
									</ul>
								<li><strong>Unsupervised</strong>: based on no training data at all.</li>
									<ul>
										<li>E.g. <a href="https://jamesgawley.github.io/Unsupervised-Lemmatization-Model/" target="_blank" title="Opens in new tab">Word Frequency</a></li>
									</ul>
								<li style="list-style: none; margin-top: 10px">Output: lemmatisation <span style="color: #a5be00">models</span>.</li>
							</ul>
							<br />
						</ol>
					</section>
				</section>-->

				</section>

